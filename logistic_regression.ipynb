{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris(as_frame=True)\n",
    "# We only take two label.\n",
    "X = iris.data.loc[iris[\"target\"] !=2].values\n",
    "Y = iris.target.loc[iris[\"target\"] !=2].values\n",
    "print(X[:5])\n",
    "print(Y[:5])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with greadient descent optimizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression model as: h(x) = θ x\n",
    "Apply sigmoid function to output of it, make it become logistic regression. \n",
    "\n",
    "h(x) = sigmoid(θ x)\n",
    "\n",
    "For loss function, we apply log_loss to calculate loss.\n",
    "The idea is to give a lot of penalties when the model predicts the wrong label.\n",
    "e.g. When actual class is 1 and the model predicts 0, we make loss very high.\n",
    "We can apply the characteristic of log in 0~1 to achieve this situation.\n",
    "\n",
    "\n",
    "So, there are two scenario for loss:\n",
    "    1. -log(h(x)),     if y=1\n",
    "    2. -log(1 - h(x)), if y=0\n",
    "Combine these two, loss function will become: -ylog(h(x)) - (1-y)log(1-h(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "    Implement Logistic Regression with greadient descent optimizer.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.1, steps=100000, verbose=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.steps = steps\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        # weights initialization\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        \n",
    "        for i in range(self.steps):\n",
    "            h = self.__sigmoid(np.dot(X, self.theta))\n",
    "            gradient = np.dot(X.T, (h - Y)) / Y.size\n",
    "            self.theta -= self.learning_rate * gradient\n",
    "            \n",
    "            if(self.verbose == True and i % 10000 == 0):\n",
    "                print(f'Steps {i} loss: {self.__loss(h, Y)} \\t')\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_prob(X) >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_model = LogisticRegression(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 0 loss: 0.6931471805599453 \t\n",
      "Steps 10000 loss: 0.0010175730409509643 \t\n",
      "Steps 20000 loss: 0.0005433929292002089 \t\n",
      "Steps 30000 loss: 0.00037613201159461627 \t\n",
      "Steps 40000 loss: 0.00028953642230122704 \t\n",
      "Steps 50000 loss: 0.00023624895979490988 \t\n",
      "Steps 60000 loss: 0.00020001362264952146 \t\n",
      "Steps 70000 loss: 0.00017370769244357857 \t\n",
      "Steps 80000 loss: 0.00015370564795079737 \t\n",
      "Steps 90000 loss: 0.00013796288471905734 \t\n"
     ]
    }
   ],
   "source": [
    "LR_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        12\n",
      "  versicolor       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        20\n",
      "   macro avg       1.00      1.00      1.00        20\n",
      "weighted avg       1.00      1.00      1.00        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = LR_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with adagrad optimizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea of Adagrad is to make the weights that have low gradient or infrequent updates should speed up learning rate and make them train faster. On the other hand, the weights that have higher gradient or frequent updates should slow the learning rate and make them not miss the minimum value.\n",
    "\n",
    "In each step, the current gradient divides the sum of squares of all previous gradients of the weight and multiple learning rate. And theta(weight) will keep updating by minus equal to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionAdagrad:\n",
    "    \"\"\"\n",
    "    Implement Logistic Regression with adagrad optimizer.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.1, steps=100000, verbose=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.steps = steps\n",
    "        self.verbose = verbose\n",
    "        self.epsilon = 0.0001\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        # weights initialization\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        gradient_sums = np.zeros(self.theta.shape[0]) # The sum of previous gradient in each t\n",
    "        for i in range(self.steps):\n",
    "            h = self.__sigmoid(np.dot(X, self.theta))\n",
    "            # adagrad optimizer\n",
    "            gradient = np.dot(X.T, (h - Y)) / Y.size\n",
    "            gradient_sums += gradient ** 2\n",
    "            gradient_update = gradient / (np.sqrt(gradient_sums + self.epsilon))\n",
    "            self.theta -= self.learning_rate * gradient_update\n",
    "            \n",
    "            if(self.verbose == True and i % 10000 == 0):\n",
    "                print(f'Steps {i} loss: {self.__loss(h, Y)} \\t')\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_prob(X) >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_model_with_adagrad = LogisticRegressionAdagrad(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 0 loss: 0.6931471805599453 \t\n",
      "Steps 10000 loss: 0.0015863726221965239 \t\n",
      "Steps 20000 loss: 0.0008505624309374706 \t\n",
      "Steps 30000 loss: 0.0005902295684958219 \t\n",
      "Steps 40000 loss: 0.00045520633002131257 \t\n",
      "Steps 50000 loss: 0.00037200171218030947 \t\n",
      "Steps 60000 loss: 0.00031535517587557074 \t\n",
      "Steps 70000 loss: 0.00027418791515945495 \t\n",
      "Steps 80000 loss: 0.0002428562289721353 \t\n",
      "Steps 90000 loss: 0.00021817512467375995 \t\n"
     ]
    }
   ],
   "source": [
    "LR_model_with_adagrad.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        12\n",
      "  versicolor       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        20\n",
      "   macro avg       1.00      1.00      1.00        20\n",
      "weighted avg       1.00      1.00      1.00        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = LR_model_with_adagrad.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression by Sklearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        12\n",
      "  versicolor       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        20\n",
      "   macro avg       1.00      1.00      1.00        20\n",
      "weighted avg       1.00      1.00      1.00        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
